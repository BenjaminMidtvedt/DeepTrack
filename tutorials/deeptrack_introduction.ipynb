{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds the module to path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepTrack\n",
    "\n",
    "This notebook gives an overview of the capabilities of DeepTrack, how to model optical systems using predifined classes, and how to implement your own classes.\n",
    "\n",
    "## What is DeepTrack?\n",
    "\n",
    "DeepTrack is fundamentally a framework for generating images. These images may be the output of optical systems, but the framework is capable of generating any image. The main idea of DeepTrack is that any image can be viewed as a series of features that takes the an image and updates it according to some rule. A feature can, for example, be adding a particle, introducing some noise, or imaging something through a optical device.\n",
    "\n",
    "### Features and properties\n",
    "\n",
    "Features in DeepTrack are classes implementing the Feature class. The way a feature updates an image is governed by the values passed to the class constructor. These inputs are converted to [properties](properties_example.ipynb). An example of a property would be the position of a particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepTrack.scatterers import PointParticle\n",
    "\n",
    "particle = PointParticle(\n",
    "    position=(0, 0),\n",
    "    position_unit=\"pixel\", # Defaults to meter\n",
    "    intensity=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a feature (a point particle). This feature will always add a point particle at x=0, y=0. For machine learning, it may be more useful to add a particle at a random position. Then one can instead pass a function that returns a random pair of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "particle = PointParticle(\n",
    "    position=lambda: np.random.rand(2) * 64,\n",
    "    position_unit=\"pixel\",\n",
    "    intensity=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolving an image\n",
    "\n",
    "To create an image from a feature, just call its method `.resolve()` with an empty ndarray (all elements zero). To update the properties (here, retrieving a new random position), we call the method `.update()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_image = np.zeros((64,64))\n",
    "\n",
    "# Retrieve a new position\n",
    "particle.update()\n",
    "\n",
    "# Add the particle to the image\n",
    "output_image = particle.resolve(input_image)\n",
    "\n",
    "plt.imshow(output_image)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output doesn't look much like a point particle. This is because it's not imaged through an optical device. Optical devices are also features, which convolves the input image with a pupil function. They also pass optical properties to features used to generate its input.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepTrack.optics import OpticalDevice\n",
    "\n",
    "optics = OpticalDevice(\n",
    "    NA=0.8,\n",
    "    wavelength=680e-9,\n",
    "    pixel_size=100e-9\n",
    ")\n",
    "\n",
    "# To image a feature, we call optics with the feature\n",
    "imaged_particle = optics(particle)\n",
    "\n",
    "imaged_particle.update()\n",
    "output_image = imaged_particle.resolve(input_image)\n",
    "\n",
    "\n",
    "plt.imshow(output_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more features\n",
    "\n",
    "Features can be combined (as detailed [here](features_example.ipynb)) using overloaded operators (+, \\*, \\*\\* or ()). Here exemplify the add operator (+) and the power opertor (\\*\\*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The + operator\n",
    "particle_1 = PointParticle(\n",
    "    position=lambda: np.random.rand(2) * 64,\n",
    "    position_unit=\"pixel\",\n",
    "    intensity=100\n",
    ")\n",
    "\n",
    "particle_2 = PointParticle(\n",
    "    position=lambda: np.random.rand(2) * 64,\n",
    "    position_unit=\"pixel\",\n",
    "    intensity=100\n",
    ")\n",
    "\n",
    "# two_particles is a new feature that first resolves particle_1 and then particle_2, then images it\n",
    "two_particles = optics(particle_1 + particle_2)\n",
    "\n",
    "output_image = two_particles.resolve(input_image)\n",
    "\n",
    "plt.imshow(output_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ** operator\n",
    "\n",
    "particle = PointParticle(\n",
    "    position=lambda: np.random.rand(2) * 64,\n",
    "    position_unit=\"pixel\",\n",
    "    intensity=100\n",
    ")\n",
    "\n",
    "# five_particles is a feature that resolves five deep copies of particle, then images it\n",
    "five_particles = optics(particle**5)\n",
    "\n",
    "output_image = five_particles.resolve(input_image)\n",
    "\n",
    "plt.imshow(output_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-padding the input\n",
    "\n",
    "You may have noticed that particles close to edges wrap around to the other side. This is due to the wrap around effect of fourier transform used when convolving an image with a pupil. Two get around this, we zero-pad the input. The desired size can be retrieved in the optics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optics = OpticalDevice(\n",
    "    NA=0.8,\n",
    "    wavelength=680e-9,\n",
    "    pixel_size=100e-9,\n",
    "    ROI = (0, 0, 64, 64) # row, column, height, width\n",
    ")\n",
    "\n",
    "five_particles = optics(particle**5)\n",
    "\n",
    "# Input a slightly larger image as zero padding\n",
    "input_image = np.zeros((96,96))\n",
    "\n",
    "output_image = five_particles.resolve(input_image)\n",
    "\n",
    "plt.imshow(output_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding noise\n",
    "\n",
    "To make the image more realistic, we can add some noise. Noise can be wrapped by an optical device, but is typically not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepTrack.noises import Offset, Poisson\n",
    "\n",
    "# Adds a constant value to the background\n",
    "offset = Offset(offset=10)\n",
    "\n",
    "# Introduces poisson noise to the image\n",
    "poisson_noise = Poisson(SNr=100)\n",
    "\n",
    "# noisy_particles resolves five particles, then adds a offset, images it, then introduces poisson noise\n",
    "noisy_particles = optics(particle**5 + offset) + poisson_noise\n",
    "\n",
    "output_image = noisy_particles.resolve(input_image)\n",
    "\n",
    "plt.imshow(output_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving information about the image\n",
    "\n",
    "To train a supervised machine learning model, you need labled images. When a features is resolved, it automatically stores the properties of all features used to create the image. This allows us to extract information about the image to use to train machine learning models.\n",
    "\n",
    "Here we extract the position of all the particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positions(image):\n",
    "    # All properties are stored in the `properties` field of the output.\n",
    "    positions = [property_dict[\"position\"] for property_dict in image.properties if \"position\" in property_dict]\n",
    "    return np.array(positions)\n",
    "\n",
    "\n",
    "noisy_particles.update()\n",
    "output_image = noisy_particles.resolve(input_image)\n",
    "\n",
    "positions = get_positions(output_image)\n",
    "\n",
    "plt.imshow(output_image)\n",
    "plt.scatter(positions[:,0], positions[:,1], c=\"r\", marker=\"x\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping features in generators\n",
    "\n",
    "Generators are ways to continuously resolve new images, and are the prefered interface to machine learning models. The default generator is defined in the Generators module. We can also optionally pass a label function to call on every image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepTrack.generators import Generator\n",
    "\n",
    "generator = Generator().generate(noisy_particles, get_positions, shape=(96,96))\n",
    "\n",
    "for _ in range(4):\n",
    "    # Outputs shape (1, height, width, 1)\n",
    "    next_image, positions = next(generator)\n",
    "    plt.imshow(np.squeeze(next_image))\n",
    "    plt.scatter(positions[0,:,0], positions[0,:,1], c=\"r\", marker=\"x\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, training a model, complete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepTrack.scatterers import PointParticle\n",
    "from DeepTrack.noises import Offset, Poisson\n",
    "from DeepTrack.optics import OpticalDevice\n",
    "from DeepTrack.generators import Generator\n",
    "from DeepTrack.models import DeepTrackNetwork\n",
    "\n",
    "# DEFINE FEATURES\n",
    "optics = OpticalDevice(NA=0.8, wavelength=680e-9, pixel_size=100e-9, ROI=(0,0,64,64))\n",
    "\n",
    "particle = PointParticle(\n",
    "    position=lambda: np.random.rand(2)*64,\n",
    "    position_unit=\"pixel\",\n",
    "    intensity=100\n",
    ")\n",
    "\n",
    "offset = Offset(\n",
    "    offset = lambda: np.random.rand()*20\n",
    ")\n",
    "\n",
    "poisson_noise = Poisson(SNr=np.linspace(50,100))\n",
    "\n",
    "training_set = optics(particle + offset) + poisson_noise\n",
    "\n",
    "\n",
    "# DEFINE LABEL FUNCTION\n",
    "def get_position(image):\n",
    "    for propertydict in image.properties:\n",
    "        if \"position\" in propertydict:\n",
    "            return propertydict[\"position\"] / 64\n",
    "        \n",
    "# DEFINE MODEL\n",
    "tracker = DeepTrackNetwork(input_shape=(64,64,1), number_of_outputs=2)\n",
    "\n",
    "# DEFINE GENERATOR\n",
    "generator = Generator().generate(training_set, get_position, shape=(96,96), batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN TRACKER\n",
    "tracker.fit(generator, epochs=100, steps_per_epoch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, labels = next(generator)\n",
    "predictions = tracker.predict(batch) * 64\n",
    "for image, position, prediction in zip(batch, labels, predictions):\n",
    "    plt.gray()\n",
    "    plt.imshow(image[:,:,0])\n",
    "    plt.scatter(position[0], position[1], c='g', marker='x')\n",
    "    plt.scatter(prediction[0], prediction[1], marker='o', facecolors=None, edgecolors='b')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
