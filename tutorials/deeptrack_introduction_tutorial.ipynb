{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds the module to path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepTrack\n",
    "\n",
    "This notebook gives an overview of the capabilities of DeepTrack and how to model optical systems using predifined classes.\n",
    "\n",
    "### What is DeepTrack?\n",
    "\n",
    "DeepTrack is fundamentally permits to create, train and execute particle tracking models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate images\n",
    "\n",
    "A central capability is to generate images, which can be used to train and validate the operation of particle tracking models. For example, these images may be like the output of optical systems employed in experiments. \n",
    "\n",
    "The main idea of DeepTrack is that any image can be viewed as a series of features. These features take an input image and update it according to some update rule. For example, a feature can add a particle, introduce some noise, or image something through an optical device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and properties\n",
    "\n",
    "Features in DeepTrack are classes implementing the class `Feature`. The way a feature updates an image is governed by the values passed to the class constructor. These inputs are converted to [properties](../examples/properties_example.ipynb). For example, a property could be the position of a particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.scatterers import PointParticle\n",
    "\n",
    "particle = PointParticle(\n",
    "    position=(0, 0),\n",
    "    position_unit=\"pixel\", # Defaults to meter\n",
    "    intensity=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a feature (a point particle). This feature will always add a point particle at x=0, y=0. For machine learning, it may be more useful to add a particle at a random position. Thsi can be done by passing a lambda function that returns a random pair of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "particle = PointParticle(\n",
    "    position=lambda: np.random.rand(2) * 64,\n",
    "    position_unit=\"pixel\",\n",
    "    intensity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A point scatterer on its own does not make much sense. We also need to define the optical system it is viewed through. Optical devices are also features, which convolves the input image with a pupil function. Here we will use a fluorescence microscope. \n",
    "\n",
    "By calling the feature `optics` with the scatterer `particle`, we create a new feature which resolves images of the particle as seen through the fluorescence microscope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.optics import Fluorescence\n",
    "\n",
    "optics = Fluorescence(\n",
    "    NA=0.8,\n",
    "    wavelength=680e-9,\n",
    "    magnification=10,\n",
    "    resolution=1e-6,\n",
    "    output_region=(0, 0, 64, 64)\n",
    ")\n",
    "\n",
    "imaged_particle = optics(particle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more features\n",
    "\n",
    "Features can be combined (see [features_example](features_example.ipynb)) using overloaded operators (+, \\*, \\*\\* or ()). Here exemplify the add operator (+) and the power operator (\\*\\*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# To image a feature, we call optics with the feature\n",
    "\n",
    "\n",
    "imaged_particle.update()\n",
    "output_image = imaged_particle.resolve()\n",
    "\n",
    "plt.imshow(output_image, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(output_image[:, :, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The + operator\n",
    "particle_1 = PointParticle(\n",
    "    position=lambda: np.random.rand(2) * 64,\n",
    "    position_unit=\"pixel\",\n",
    "    intensity=1\n",
    ")\n",
    "\n",
    "particle_2 = PointParticle(\n",
    "    position=lambda: np.random.rand(2) * 64,\n",
    "    position_unit=\"pixel\",\n",
    "    intensity=1\n",
    ")\n",
    "\n",
    "# two_particles is a new feature that first resolves particle_1 and then particle_2, then images it\n",
    "two_particles = optics(particle_1 + particle_2)\n",
    "\n",
    "output_image = two_particles.resolve()\n",
    "\n",
    "plt.imshow(output_image[:, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ** operator\n",
    "\n",
    "particle = PointParticle(\n",
    "    position=lambda: np.random.rand(2) * 64,\n",
    "    position_unit=\"pixel\",\n",
    "    intensity=1\n",
    ")\n",
    "\n",
    "# five_particles is a feature that resolves five deep copies of particle, then images it\n",
    "five_particles = optics(particle**5)\n",
    "\n",
    "output_image = five_particles.resolve()\n",
    "\n",
    "plt.imshow(output_image[:, :, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding noise\n",
    "\n",
    "To make the image more realistic, we can add some noise. Noise can be added before (most commonly) or after applying the optical device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.noises import Offset, Poisson\n",
    "\n",
    "# Adds a constant value to the background\n",
    "offset = Offset(offset=0.01)\n",
    "\n",
    "# Introduce Poisson noise to the image\n",
    "poisson_noise = Poisson(snr=100)\n",
    "\n",
    "# noisy_particles resolves five particles, then adds a offset, images it, then introduces poisson noise\n",
    "noisy_particles = optics(particle**5) + offset + poisson_noise\n",
    "\n",
    "output_image = noisy_particles.resolve()\n",
    "\n",
    "plt.imshow(output_image[:, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve information about the image\n",
    "\n",
    "To train a supervised machine-learning model, labled images are needed. When a features is resolved, it automatically stores the properties of all features used to create the image. This allows us to extract information about the image in order to use them to train machine-learning models.\n",
    "\n",
    "Here, we extract the position of all the particles and plot them as red crossed on the generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positions(image):\n",
    "    # All properties are stored in the `properties` field of the output.\n",
    "    positions = [property_dict[\"position\"] for property_dict in image.properties if \"position\" in property_dict]\n",
    "    return np.array(positions)\n",
    "\n",
    "\n",
    "\n",
    "noisy_particles.update()\n",
    "output_image = noisy_particles.resolve()\n",
    "\n",
    "positions = get_positions(output_image)\n",
    "\n",
    "plt.imshow(output_image[:, :, 0], cmap='gray')\n",
    "plt.scatter(positions[:, 1], positions[:, 0], c=\"r\", marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wrap features in generators\n",
    "\n",
    "Generators are ways to continuously resolve new images, and are the prefered interface to machine learning models. The default generator is defined in the module `generators`. We can also optionally pass a label function that will be called on every image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.generators import Generator\n",
    "\n",
    "generator = Generator().generate(noisy_particles, get_positions)\n",
    "\n",
    "for _ in range(4):\n",
    "    # Outputs shape (1, height, width, 1)\n",
    "    next_image, positions = next(generator)\n",
    "    plt.imshow(np.squeeze(next_image), cmap='gray')\n",
    "    plt.scatter(positions[0, :, 1], positions[0, :, 0], c=\"r\", marker=\"x\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.models import convolutional\n",
    "from deeptrack.math import NormalizeMinMax\n",
    "\n",
    "# DEFINE LABEL FUNCTION\n",
    "def get_position(image):\n",
    "    for propertydict in image.properties:\n",
    "        if \"position\" in propertydict:\n",
    "            return propertydict[\"position\"] / 64\n",
    "        \n",
    "# DEFINE MODEL\n",
    "tracker = convolutional(input_shape=(64, 64, 1), number_of_outputs=2)\n",
    "\n",
    "\n",
    "# DEFINE TRAINING SET\n",
    "normalization = NormalizeMinMax(min=0, max=1)\n",
    "training_set = optics(particle) + offset + poisson_noise + normalization\n",
    "\n",
    "# DEFINE GENERATOR\n",
    "generator = Generator().generate(training_set, get_position, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN TRACKER\n",
    "tracker.fit(generator, epochs=100, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, labels = next(generator)\n",
    "predictions = tracker.predict(batch) * 64\n",
    "for image, position, prediction in zip(batch, labels * 64, predictions):\n",
    "    plt.gray()\n",
    "    plt.imshow(image[:, :, 0], cmap='gray')\n",
    "    print(np.max(batch))\n",
    "    plt.scatter(position[1], position[0], c='g', marker='x')\n",
    "    plt.scatter(prediction[1], prediction[0], marker='o', facecolors=None, edgecolors='b')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
