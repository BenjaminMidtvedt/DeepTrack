{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepTrack - Distinguishing particles in brightfield\n",
    "\n",
    "This notebook demonstrates how to use a U-NET to track and distinguish particles of different sizes in brightfield.\n",
    "\n",
    "This tutorial should be read after the tutorials [deeptrack_introduction_tutorial](deeptrack_introduction_tutorial.ipynb) and [tracking_multiple_particles_unet_tutorial](tracking_multiple_particles_unet_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Imports needed for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptrack.scatterers import Ellipse\n",
    "from deeptrack.optics import Brightfield, IlluminationGradient\n",
    "from deeptrack.noises import Poisson\n",
    "from deeptrack.generators import Generator\n",
    "from deeptrack.models import unet\n",
    "from deeptrack.aberrations import Zernike\n",
    "from deeptrack.augmentations import FlipLR, FlipUD, FlipDiagonal\n",
    "from deeptrack.math import NormalizeMinMax\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the particles\n",
    "\n",
    "For this example, we consider circular disk particles of two different sizes. The particles are instances of the class `Ellipse`, which takes the following inputs:\n",
    "\n",
    "* position: The in-sample position of the particle.\n",
    "* z (optional): The distance of the particle from the focal plane in the direction of the camera. \n",
    "* position_unit: \"pixel\" or \"meter\", defines the scale factor of the particle position\n",
    "* radius: The radius of the particle in meters. If two numbers, they define the dimensions of an ellipse.\n",
    "* rotation (optional): If elliptical, this defines the angle between the first axis of the particle and the x-axis.\n",
    "* refractive_index / intensity / value: The characteristics of the particle used by the optical system.\n",
    "\n",
    "The particles' are defined to be randomly sampled between 0 and 256 pixels in the camera plane, and within -10 to 10 pixel units from the focal plane. The smaller particle has a radius between 200-250 nm, while the larger has a radius between 400-500 nm. Their refractive index are both 0.12, with an absorption coefficient between 0.1 and 0.15, determined by the imaginary part of the refractive index. Finally we define a dummy-property, `particle_type`, which helps us distinguish between the particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_particle = Ellipse(\n",
    "    position=lambda: np.random.rand(2) * 256,\n",
    "    z=lambda:np.random.rand() * 20 - 10,\n",
    "    position_unit=\"pixel\", # Defaults to meter\n",
    "    radius=lambda: np.random.rand() * 0.05e-6 + 0.2e-6,\n",
    "    refractive_index=lambda: 0.12 + (0.1j + np.random.rand() * 0.05j),\n",
    "    particle_type = 0\n",
    ")\n",
    "\n",
    "large_particle = Ellipse(\n",
    "    position=lambda: np.random.rand(2) * 256,\n",
    "    z=lambda:np.random.rand() * 20 - 10,\n",
    "    position_unit=\"pixel\", # Defaults to meter\n",
    "    radius=lambda: np.random.rand() * 0.1e-6 + 0.4e-6,\n",
    "    refractive_index=lambda: 0.12 + (0.1j + np.random.rand() * 0.05j),\n",
    "    particle_type = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the optics \n",
    "\n",
    "Next, we need to define the properties of the optical system. Here, we use the class `Brightfield`, which uses particles with a defined refractive_index and illuminates them coherently. It takes the following inputs:\n",
    "\n",
    "* wavelength: The wavelength of the illuminting light (meters)\n",
    "* NA: The NA of the limiting aperature\n",
    "* resolution: The pixel density in the camera (meters)\n",
    "* magnification: The magnification of the aperature\n",
    "* output_region: The position and size of the camera in pixels (x, y, pixels_x, pixels_y)\n",
    "* illumination: The light illuminating to the sample, if undefined, the sample is illuminated homogenously with intensity 1.\n",
    "\n",
    "To simulate incoherent light, we define multiple optical devices in a range of wavelengths from 400-700 nm. The number of optical devices is a trade-off between accuracy and speed. Moreover, we create an instance of the class `IlluminationGradient`, which adds an intensity gradient to the illuminating light. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illumination = IlluminationGradient(gradient=lambda: np.random.randn(2) * 0.2)\n",
    "\n",
    "spectrum = np.linspace(400e-9, 700e-9, 5)\n",
    "optics = [Brightfield(\n",
    "            wavelength=wavelength,\n",
    "            NA=0.95,\n",
    "            resolution=1e-6,\n",
    "            magnification=10,\n",
    "            illumination=illumination,\n",
    "            output_region=(0, 0, 256, 256))\n",
    "          for wavelength in spectrum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define noises\n",
    "\n",
    "The noise in the system is Poisson distributed with SNR between 50 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = Poisson(snr=lambda: 50 + np.random.rand() * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine the features \n",
    "\n",
    "The sample is defined as a between 5 and 24 small particles, and between 5 and 24 large particles. This is illuminated with the bright-field optics defined above. We use the python function `sum()` to add the optical devices together. This is equivalent to the more verbose statement\n",
    "\n",
    "`optics[0](sample) + optics[1](sample) + ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_small_particles = lambda: np.random.randint(5, 25)\n",
    "number_of_large_particles = lambda: np.random.randint(5, 25)\n",
    "\n",
    "sample = small_particle**number_of_small_particles + large_particle**number_of_large_particles\n",
    "\n",
    "incoherently_illuminated_sample = sum([coherent_optics(sample) for coherent_optics in optics])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Augmenting the images\n",
    "\n",
    "Simulating many optical devices is slow. The speed up the training, we use augmentation. These are special features that allow you to resolve more images before resolving the base feature (`coherently_illuminated_image`). We use 3 augmentations, FlipLR, which mirrors the image left to right, FlipUD, which mirrors the image up to down, and FlipDiagonal, which mirrors the image along the main diagonal. This results in an 8-fold increase in images. \n",
    "\n",
    "We add the noise after the augmentation as a cheap way to make the images more different.\n",
    "\n",
    "Finally we normalize the images using NormalizeMinMax, which transforms the images to values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_image = FlipUD(FlipLR(FlipDiagonal(incoherently_illuminated_sample)))\n",
    "\n",
    "image_of_particles = augmented_image + noise + NormalizeMinMax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot example images\n",
    "\n",
    "Now, we visualize some example images. At each iteration, we call the method `.update()` to refresh the random features in the image (particle number, particle position, Poisson noise etc.). Afterwards we call the method `.plot()` to generate and display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    image_of_particles.update()\n",
    "    image_of_particles.plot(cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create the target images\n",
    "\n",
    "We define a function that uses the generated images to create the target images to be used in the training. The target is a image of shape (256, 256, 3), where the last dimension represents three classes. In other words, each pixel in the input image is classified into one of three classes. The first class is the null-class, that is, the class of a pixel that belongs to no other class. The second class is 1 if the pixel is within 3 pixels of the center of a small particle, while the third class is 1 if the pixel is within 3 pixels of a large particle.\n",
    "\n",
    "We also show images and targets side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_image(image_of_particles):\n",
    "    label = np.zeros((*image_of_particles.shape[:2], 3))\n",
    "    \n",
    "    \n",
    "    X, Y = np.meshgrid(\n",
    "        np.arange(0, image_of_particles.shape[0]), \n",
    "        np.arange(0, image_of_particles.shape[1])\n",
    "    )\n",
    "\n",
    "    for property in image_of_particles.properties:\n",
    "        if \"position\" in property:\n",
    "            position = property[\"position\"]\n",
    "            distance_map = (X - position[1])**2 + (Y - position[0])**2\n",
    "            label[distance_map < 9, property[\"particle_type\"] + 1] = 1\n",
    "            \n",
    "    label[..., 0] = 1 - np.max(label[..., 1:], axis=-1)\n",
    "    \n",
    "    return label\n",
    "\n",
    "# Show images\n",
    "for i in range(4):\n",
    "    augmented_image.update()\n",
    "    \n",
    "    image_of_particles = augmented_image.resolve()\n",
    "    \n",
    "    label_of_particles = get_target_image(image_of_particles)\n",
    "    \n",
    "    plt.figure(figsize=(12, 9))\n",
    "    \n",
    "    plt.subplot(1, 4,1)\n",
    "    plt.imshow(image_of_particles[..., 0], cmap=\"gray\")\n",
    "    \n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(label_of_particles[..., 0], cmap=\"gray\")\n",
    "    \n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(label_of_particles[..., 1], cmap=\"gray\")\n",
    "    \n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(label_of_particles[..., 2], cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define image generator\n",
    "\n",
    "We define a generator that creates images and targets in batches of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().generate(\n",
    "    augmented_image, \n",
    "    get_target_image,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define loss function\n",
    "\n",
    "We also define a custom loss function. The loss function is binary crossentropy, where each class is wighted by 1-p, where p is is the proportion of all pixels in that class which is 1 in the label. This helps us avoid the local minima of classifying all pixels as the null-class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "eps = 1e-6\n",
    "def softmax_categorical(T, P):\n",
    "    classwise_weight = K.mean(1 - T, axis=(1, 2), keepdims=True)\n",
    "    true_error = K.mean(T * K.log(P + eps) * classwise_weight, axis=-1)\n",
    "    return -K.mean(true_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Define the neural network model\n",
    "\n",
    "The neural network architecture used is a U-Net, which is a fully convoltional model used for image to image transformations. We add a softmax activation to the final layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(\n",
    "    (256, 256, 1), \n",
    "    conv_layers_dimensions=[8, 16, 32],\n",
    "    base_conv_layers_dimensions=[32, 32], \n",
    "    number_of_outputs=3,\n",
    "    output_activation=\"softmax\",\n",
    "    loss=softmax_categorical\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train the model\n",
    "\n",
    "The model is trained by calling `.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(generator,\n",
    "          epochs=100,\n",
    "          steps_per_epoch=10,\n",
    "          max_queue_size=0,\n",
    "          workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize the model performance\n",
    "\n",
    "Finally we evaluate the model performance by showing the model output beside the input image and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().generate(\n",
    "    augmented_image, \n",
    "    get_target_image,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "input_image, target_image = next(generator)\n",
    "\n",
    "\n",
    "for i in range(input_image.shape[0]):\n",
    "    \n",
    "    predicted_image = model.predict(input_image)\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(input_image[i, :, :, 0], cmap=\"gray\")\n",
    "    plt.title(\"Input Image\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(predicted_image[i, :, :, 1], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(\"Predicted Image\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(predicted_image[i, :, :, 2], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(\"Predicted Image\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(target_image[i, :, :, 0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(target_image[i, :, :, 1], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(target_image[i, :, :, 2], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(\"Ground Truth\")\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
